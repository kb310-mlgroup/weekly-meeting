{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y_train one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 105) (4, 45) (105,) (3, 105) (45,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, shuffle=True)\n",
    "X_train = X_train.transpose()\n",
    "X_test = X_test.transpose()\n",
    "Y_train_one_hot = np.zeros((3, len(Y_train)))\n",
    "for i in range(len(Y_train)):\n",
    "    Y_train_one_hot[Y_train[i], i] = 1\n",
    "Y_test = Y_test.transpose()\n",
    "print(X_train.shape, X_test.shape, Y_train.shape,Y_train_one_hot.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数值安全 $$ a = \\sigma(z) = \\frac{\\exp(z_i-c)}{\\sum_j\\exp(z_j - c)} $$\n",
    "\n",
    "$$ c = \\max_iz_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\". \n",
    "    if activation == 'relu':\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        A = np.maximum(0, Z)\n",
    "    \n",
    "    elif activation == 'softmax':\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        C = np.max(Z, axis = 0)\n",
    "        A = np.exp(Z - C) / np.sum(np.exp(Z - C), axis = 0, keepdims=True) #数值稳定\n",
    "    cache = (A_prev, W, b, Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], 'relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> softmax. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], 'softmax')  \n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (3,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数使用交叉熵损失函数：\n",
    "$$ L = - \\frac{1}{m}\\sum_i\\sum_ky^i_k\\log a^i_k $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = - np.sum(np.log(np.sum(AL*Y, axis=0))) / m\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$ \n",
    "$dZ^{[l]}$.Here are the formulas you need:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.transpose()) / m\n",
    "    db = np.sum(dZ, axis = 1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.transpose(), dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $g(.)$ is the activation function, \n",
    "`relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation='relu'):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b, Z = cache\n",
    "    \n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    dA_prev, dW, db = linear_backward(dZ, A_prev, W, b)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax梯度\n",
    " $ a_i = \\frac{e^{z_i}}{\\sum_je^{z_j}} $   $$ L = - \\sum_ky_k\\log{a_k} $$  \n",
    "  当 $i = j$ 时 \n",
    "$$ \\frac{da_i}{dz_j} = \\frac{e^{a_i\\sum_kle^{z_k} - e^{z_i}e^{z_i}}}{(\\sum_ke^{z_k})^2} = a_i(1 - a_i)$$ \n",
    "  当 $ i\\neq j $ 时\n",
    " $$ \\frac{da_i}{dz_j} = - \\frac{e^{a_i}e^{a_j}}{(\\sum_ke^{z_k})^2} = -a_ia_j $$\n",
    " \n",
    "$$ \\frac{d_L}{dz_i} = - \\sum_ky_k\\frac{1}{a_k}\\frac{da_k}{dz_i} = -\\frac{y_i}{a_i}a_i(1-a_i) + \\sum_{k\\neq i}\\frac{y_k}{a_k}a_ia_k = -y_i + y_ia_i + \\sum_{k\\neq i}y_ka_i $$ $$= -y_i + a_i\\sum_ky_k = a_i - y_i$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_backward(dAL, AL):\n",
    "    n = AL.shape[0]\n",
    "    dA_dZ = np.zeros((n, n))\n",
    "    dZ = np.zeros(AL.shape)\n",
    "    for l in range(AL.shape[1]):\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i == j:\n",
    "                    dA_dZ[i, j] = AL[i, l]*(1 - AL[i, l])\n",
    "                else: \n",
    "                    dA_dZ[i, j] = - AL[i, l]*AL[j, l]\n",
    "#         print(dA_dZ)\n",
    "        dZ[:,l] = np.dot(dA_dZ, dAL[:, l])\n",
    "    return dZ\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.33333333]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n",
      "[[-0.7]\n",
      " [ 0.2]\n",
      " [ 0.5]]\n",
      "[[-0.7]\n",
      " [ 0.2]\n",
      " [ 0.5]]\n",
      "[[ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([[1], [0], [0]])\n",
    "AL = np.array([[0.3], [0.2], [0.5]])\n",
    "dAL = - Y/ AL\n",
    "print(dAL)\n",
    "dZ = softmax_backward(dAL, AL)\n",
    "print(dZ)\n",
    "print(AL - Y)\n",
    "print(dZ == AL - Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sofymax\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "#     dZl = AL - Y\n",
    "    dAL = - Y / AL\n",
    "    dZL = softmax_backward(dAL, AL)\n",
    "    assert(dZL.shape == (AL - Y).shape)\n",
    "            \n",
    "    # Lth layer (SOFTMAX -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    A_prev, W, b, Z = caches[L - 1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(dZL, A_prev, W, b)\n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation='relu')\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate*grads['dW' + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate*grads['db' + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 300, print_cost=True):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "   \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches =  L_model_forward(X, parameters)\n",
    "       \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 20 == 0:\n",
    "            costs.append(cost)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            \n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 7.348496\n",
      "Cost after iteration 20: 0.926522\n",
      "Cost after iteration 40: 0.812820\n",
      "Cost after iteration 60: 0.731598\n",
      "Cost after iteration 80: 0.666303\n",
      "Cost after iteration 100: 0.609205\n",
      "Cost after iteration 120: 0.558459\n",
      "Cost after iteration 140: 0.512316\n",
      "Cost after iteration 160: 0.470838\n",
      "Cost after iteration 180: 0.433557\n",
      "Cost after iteration 200: 0.400496\n",
      "Cost after iteration 220: 0.371463\n",
      "Cost after iteration 240: 0.346202\n",
      "Cost after iteration 260: 0.324316\n",
      "Cost after iteration 280: 0.305439\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwgnfzCwnnPDNzHLCCd/MLCf+P4SL8vn8i5RVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c4b4abc128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers_dims = [4, 6, 3] \n",
    "parameters = L_layer_model(X_train, Y_train_one_hot, layers_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    # convert probas to 0/1 predictions\n",
    "    p = probas.argmax(0) \n",
    "#     for i in range(0, probas.shape[1]):\n",
    "#            p[0,i] = \n",
    "    \n",
    "    print (\"predictions: \" + str(p))\n",
    "    print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [2 0 2 1 1 0 2 2 1 1 0 1 1 0 0 1 2 0 2 1 0 0 1 1 1 2 0 2 2 2 0 2 1 0 2 1 2\n",
      " 0 0 2 0 2 0 1 0]\n",
      "true labels: [2 0 2 1 1 0 2 2 2 1 0 2 1 0 0 1 2 0 2 1 0 0 1 1 1 2 0 2 2 2 0 2 1 0 2 1 2\n",
      " 0 0 2 0 2 0 1 0]\n",
      "Accuracy: 0.955555555556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2, 1, 1, 0, 2, 2, 1, 1, 0, 1, 1, 0, 0, 1, 2, 0, 2, 1, 0, 0, 1,\n",
       "       1, 1, 2, 0, 2, 2, 2, 0, 2, 1, 0, 2, 1, 2, 0, 0, 2, 0, 2, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X_test, Y_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
