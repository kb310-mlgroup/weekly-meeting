# 深度学习-7.8-7.11

author: long

PS：第七章为深度学习中的正则化，注意到这一章讲的都是正则化的方法。

**正则化**：对学习算法的修改——旨在减少泛化误差而不是训练误差。

## 7.8 提前终止

当训练有足够能力甚至会过拟合的大模型时，我们经常观察到，训练误差会随着时间的推移逐渐降低但验证集的误差会再次上升。

![loss][1]

可以观察到上图，随着训练轮次的增加，虽然训练误差不断减小，但是验证集上的误差在逐渐缓慢的增加，这通常是由于**过拟合**导致的。

那么我们自然就会想能否一直检查模型的验证误差，挑选验证误差最小时的模型，这个模型起码在验证集上表现最好，减小**欠拟合**或者**过拟合**的风险。这就是**提前终止**的思想。

算法如下：

![early_stop][2]

算法很简单，在训练过程中不断检查验证误差，如果连续`p`次检查训练误差都没能降低，那么就结束训练，泛化验证误差**最小**的模型，以及它的**训练步数**。

**训练步数**作为一个超参数，它是唯一只要跑一次训练就能尝试很多值得超参数。

**提前终止**需要带来以下的一些代价：

1. 在训练时需要额外再验证集上进行评估。
2. 需要保存最佳的模型，也就是需要额外的存储空间。
3. 需要从训练集中分离出部分数据作为验证集。

前两个代价基本不会有什么影响，额外的评估并不会消耗多少的时间，保存最佳模型，需要的是额外的硬盘空间，只要模型不是太大，它并不会成为问题。

对于第三个问题，**需要分离出验证集**，不能用上所有的数据来进行训练，的确有一点难受。这里有两个策略来重新利用验证集数据：

![valid][3]

![valid][4]

如上所示，**方法1**：使用提前终止得到最佳训练步数`t`后，重新使用相同的权重初始值以及全部的训练数据训练`t`步；**方法2**：提前终止训练结束后，直接使用全部数据接着训练，直到训练误差小于提前终止结束时刻的模型的最小训练误差。

当然，这两个方法都存在一些问题，对**方法1**来说，并不能确定在数据集增大的情况下，再使用`t`步还是最好的，或者说是好的。对**方法2**来说，可能训练误差永远不能低于提前终止结束时刻的模型的最小训练误差。

**提前终止为何具有正则化效果：**书上认为提前终止可以将优化过程的参数空间限制在初始参数值$\theta_0$的小领域内。

![early_stop][5]

在二次误差的简单线性模型和简单梯度下降情况下，可以推出提前终止**相当于**$L^2$正则化。

对于代价函数$J(\theta)$，将它在最佳值$w^*$处进行二阶泰勒展开，

![formula][6]

由于$w^*$为最优值，所以代价函数的梯度为0，所以上式的**一阶项**为0。同时，$H$**半正定**，因为若$H$不是半正定，那么就存在向量$x$使得$x^THx<0$，那么这时$J(x)<J(w^*)$，与$w^*$为最优值矛盾，所以在当前假设条件下，$H$一定式半正定的。

梯度为，

![formula][7]

分析$\hat{J}$上进行梯度下降的效果，

![formula][8]

将$H$进行分解，$H=Q\Lambda Q^T$，其中$\Lambda$是对角矩阵，$Q$是特征向量的一组标准正交基。

![formula][9]

![formula][10]

假定$w^{(0)}=0$，上式可以重写为，

![formula][11]

回顾$L^2$正则化时，最后推出的式子，$\widetilde{w}=Q(\Lambda +\alpha I)^{-1}\Lambda Q^T w^*$，将它重写，

![formula][12]

对比`7.40`和`7.42`两个式子，可以观察到，如果超参数$\epsilon$、$\alpha$和$\tau$满足

![formula][13]

此时，$L^2$正则化和提前终止可以看作**等价**的。也就证明了提前终止**具有正则化效果**。



## 7.9 参数绑定和参数共享

**参数绑定**：

考虑情形：有两个模型执行相同的分类任务，但输入分布稍有不同。形式地，有参数为$w^{(A)}$的模型$A$和参数为$w^{(B)}$的模型$B$。这两个模型将输入映射到两个不同但相关的输出：$\hat{y}^{(A)}=f(w^{(A)},x)$和$\hat{y}^{(B)}=f(w^{(B)},x)$。

由于这两个任务具有足够的相似性，所以我们认为模型参数应彼此靠近：$\forall i$，$w_i^{(A)}$应该与$w_i^{(B)}$接近。那么可以使用**正则化**来对它进行表达，使用以下形式的参数范数惩罚：$\Omega (w^{(A)},w^{(B)})=||w^{(A)}-w^{(B)}||_2^2$。

**参数共享**：

相比参数绑定，参数共享更加常用，它强迫某些参数相等。**卷积神经网络**就是一个使用参数共享的例子。



## 7.10 稀疏表示

在之前学习过$L^1$正则化，它可以看作一种**模型参数稀疏化**的方法，这里的稀疏表示，是将**训练数据进行稀疏化**。

**关于参数稀疏可以参考西瓜书11.5**

表示的正则化可以使用参数正则化种同种机制实现，

![formula][14]

其中$h$是$x$的稀疏表示。为了实现稀疏化，这里的$\Omega(h)$和$L^1$正则化一样，$\Omega (h)=||h||_1=\sum_i|h_i|$。

这只是其中一种方法，其它方法还包括从表示上的Student-t先验导出的惩罚和KL散度惩罚。

（学生-t分布，用于根据小样本来估计呈正态分布且方差未知的总体的均值）

（KL散度，描述两个概率分布P和Q差异的一种方法）

另外还有方法**正交匹配追踪**，通过解决以下约束优化问题将输入值$x$编码成表示h，

![formula][15]

其中$||h||_0$是$h$中非零项的个数。这种方法通常被称为**OMP-k**，通过$k$指定允许的非零特征数量。



## 7.11 Bagging和其他集成方法

本节可以参考西瓜书第8章。

**Bagging**(bootstrap aggregating)是通过结合几个模型降低泛化误差的技术。主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出，这样的方法也叫做**模型平均**。

假设我们有k个回归模型，每个模型在每个例子上的误差是$\epsilon_i$，服从零均值方差为$\mathbb{E}[\epsilon^2_i]=v$且协方差为$\mathbb{E}[\epsilon_i \epsilon_j]=c$的多维正态分布。通过所有集成模型的平均预测所得误差是$\frac{1}{k}\sum_i \epsilon_i$。集成预测器平方误差的期望是，
![formula][16]

在误差完全相关，即$c=v$的情况下，均方误差为$v$，模型平均没有帮助。在错误完全不相关的情况下，均方误差为$\frac{1}{k}v$。这意味着集成平方误差的期望会随着集成规模的增大而线性减小。

Bagging涉及构造$k$个不同的数据集，它使用**自助采样法**，从原始数据集中有放回的随机抽取$m$个样本，组成一个训练集，重复这个过程可以从原始数据集中采样出多个不同的训练集，这样不同的训练集训练出来的模型之间就会存在差异。

**模型平均**是一个减少泛化误差的非常强大可靠的方法。正因为它十分强大，在机器学习比赛中想要获胜几乎都会使用。也因此在科学论文算法的基准时，它通常是不鼓励使用的。

还有另一种不太一样的集成方法**Boosting**，可以参考西瓜书。







---------

[1]: images/loss.png	"training loss"
[2]: images/early_stop_1.png	"early stop"
[3]: images/valid_1.png	"validation use"
[4]: images/valid_2.png	"validation use"
[5]: images/early_stop_2.png	"early stop"
[6]: images/formula_1.png	"formula"
[7]: images/formula_2.png	"formula"
[8]: images/formula_3.png	"formula"
[9]: images/formula_4.png	"formula"
[10]: images/formula_5.png	"formula"
[11]: images/formula_6.png	"formula"
[12]: images/formula_7.png	"formula"
[13]: images/formula_8.png	"formula"
[14]: images/formula_9.png	"formula"
[15]: images/formula_10.png	" formula"
[16]: images/formula_11.png	"formula"

