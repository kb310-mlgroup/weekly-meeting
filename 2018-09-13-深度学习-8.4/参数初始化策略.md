# 深度学习8.4 - 参数初始化策略

在优化问题中，有两种优化算法是比较好的：

1. 非迭代的，只求解一个解点。
2. 能在可接受时间内收敛到可接受的解，并且与初始值无关。

**但是**，深度学习训练算法通常没有这两种奢侈的性质，它通常是迭代的，并且会受到初始值选取的影响。

初始值的选取会带来下面的几点影响：

1. 决定算法是否收敛。
2. 算法收敛的速度。
3. 收敛到一个代价高或低的点。
4. 影响模型泛化性能。

那么该如何选择初始值呢？**现代的初始化策略是简单的、启发式的。**由于这个问题非常的困难，而且如今大家还并不能很好的理解神经网络优化，所以并没有一个确定的好的方法来选择初始值，以至于通常我们可以把它当作一个超参数来处理，使用不同的初始化方法来达到不同的效果。



## 最基本的准则

即使不能告诉你该选哪一个初始值，但是也至少能告诉你哪些初始值**最好不要选**。

**第一，“破坏对称性”**。如果具有相同的激活函数的两个隐藏单元连接到相同的输入，那么这些单元必须具有不同的初始参数。如果它们具有相同的初始参数，然后应用到确定性损失和模型的确定性学习算法将一直以相同的方式更新这两个单元。（当然训练算法使用一些带随机性的方法，例如Dropout，来解决这个问题，但并不建议这么做。）

**第二，更大的权重。**更大的初始权重具有更强的破坏对称性的作用，有助于避免冗余的单元，也有助于避免在每层线性成分的前向或反向传播中丢失信号。**但是**，过大的权重会在前向传播或反向传播中产生爆炸的值。

**第三，均值接近0。**如果我们初始化$\theta_0$为很大的值，那么我们的先验指定了哪些单元应互相交互，以及它们应如何交互。所以通常选择$\theta_0$接近0，这个先验表明单元间彼此互不交互比交互更有可能。（这一段不好懂，可以查看书中原文）

**所以**，通过上面的三条基本准则，我们几乎总是初始化模型的权重为**高斯**或**均匀分布**中随机抽取的值，并且通过一些准则来确定分布的范围。

下面简单说以下不同的初始化策略。



## 初始化策略

初始化神经网络m个输入，n个输出的全连接神经网络。

### 均匀分布初始化

方式一：

$$W_{i,j} \sim U(- \frac{1}{\sqrt{m}},\frac{1}{\sqrt{m}})$$

方式二，标准初始化(glorot初始化)：

$$W_{i,j} \sim U(- \sqrt{\frac{6}{m+n}},\sqrt{\frac{6}{m+n}})$$

### 高斯初始化

方式一，标准：

$$W_{i,j} \sim N(0, \frac{1}{\sqrt{m}})$$

方式二，`Xavier-normal`：

$$W_{i,j} \sim M(0, \sqrt{\frac{2}{m+n}})$$

### 正交初始化

`Saxe et al.(2013)`推荐初始化为随机正交矩阵，仔细挑选负责每一层非线性缩放或**增益**因子`g`。在不含非线性的矩阵相乘序列的深度网络模型下，这个初始化方案保证了达到收敛所需的训练迭代总数独立于深度。（在LSTM中可能会使用）

### 稀疏初始化

首先，数值范围准则有一个缺点，例如在设置一个层很大的网络时，由于m很大，会导致$\frac{1}{\sqrt{m}}$很小，那么初始权重也会变得很小。

`Marrens(2010)`提出了一种称为**稀疏初始化（sparse initialization）**的替代方案，每个单元初始化为恰好有`k`个非零权重。这个想法保持该单元输出的总数量独立于输入数目m，而不使单一权重元素的大小随`m`缩小。

### 机器学习初始化

可以使用机器学习初始化模型参数。



## 偏置的初始化

通常将偏置设置为**零值**是一个可能的初始化方案，但也存在一些情况下需设置为非零值

- 如果偏重是作为输出单元，那么初始化偏置以获取正确的输出边缘统计通常是有利的。
- 有时，我们可能想要选择偏置以避免初始化引起太大饱和。
- 有时，一个单元会控制其他单元能否参与到等式中。

